{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a634cbb5-87cf-48ae-9840-84b306e5c9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import os\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c3ad5a-bc9f-48b3-8ab0-3be85f15cf20",
   "metadata": {},
   "source": [
    "## Generating adamson dataset for biolord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89369e6c-207c-426f-9ab9-7db7e00a21aa",
   "metadata": {},
   "source": [
    "To generate the adamson dataset for model `biolord`, we followed its author's [jupyter notebook](https://github.com/nitzanlab/biolord_reproducibility/blob/main/notebooks/perturbations/adamson/1_perturbations_adamson_preprocessing.ipynb) of preprocessing workflows. Since they uploaded the resulting preprocessed data in [`adamson_single_biolord.h5ad`](https://figshare.com/articles/dataset/perturbseq_adamson_single/22344445) and [`adamson_biolord.h5ad`](https://figshare.com/articles/dataset/perturbseq_adamson/22344214), we can just download them using CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60dcf39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adamson/adamson_single_biolord.h5ad not found, downloading...\n",
      "adamson/adamson_biolord.h5ad not found, downloading...\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "file_1_path = \"adamson/adamson_single_biolord.h5ad\"\n",
    "file_2_path = \"adamson/adamson_biolord.h5ad\"\n",
    "\n",
    "# URLs\n",
    "file_1_url = \"https://figshare.com/ndownloader/files/39756736\"\n",
    "file_2_url = \"https://figshare.com/ndownloader/files/39756439\"\n",
    "\n",
    "# Check if file 1 exists, if not, download it\n",
    "if not os.path.exists(file_1_path):\n",
    "    print(f\"{file_1_path} not found, downloading...\")\n",
    "    urllib.request.urlretrieve(file_1_url, file_1_path)\n",
    "else:\n",
    "    print(f\"{file_1_path} already exists, skipping download.\")\n",
    "\n",
    "# Check if file 2 exists, if not, download it\n",
    "if not os.path.exists(file_2_path):\n",
    "    print(f\"{file_2_path} not found, downloading...\")\n",
    "    urllib.request.urlretrieve(file_2_url, file_2_path)\n",
    "else:\n",
    "    print(f\"{file_2_path} already exists, skipping download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb150a1d-785c-4aef-9d38-04b4850ff388",
   "metadata": {},
   "source": [
    "## Generating norman dataset for biolord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c94e1-1a1b-438b-88f5-99c1c5bea8ca",
   "metadata": {},
   "source": [
    "Similarly, to generate the norman dataset for model `biolord`, we followed its author's [jupyter notebook](https://github.com/nitzanlab/biolord_reproducibility/blob/main/notebooks/perturbations/norman/1_perturbations_norman_preprocessing.ipynb) of preprocessing workflows. Since they uploaded the resulting preprocessed data in [`norman2019_single_biolord.h5ad`](https://figshare.com/articles/dataset/pertrubseq_norman_single/22344427) and [`norman2019_biolord.h5ad`](https://figshare.com/articles/dataset/perturbseq_nornan/22344253), we can just download them using CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63a71f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norman/norman2019_single_biolord.h5ad not found, downloading...\n",
      "norman/norman2019_biolord.h5ad not found, downloading...\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "file_3_path = \"norman/norman2019_single_biolord.h5ad\"\n",
    "file_4_path = \"norman/norman2019_biolord.h5ad\"\n",
    "\n",
    "# URLs\n",
    "file_3_url = \"https://figshare.com/ndownloader/files/39756733\"\n",
    "file_4_url = \"https://figshare.com/ndownloader/files/39756463\"\n",
    "\n",
    "# Check if file 3 exists, if not, download it\n",
    "if not os.path.exists(file_3_path):\n",
    "    print(f\"{file_3_path} not found, downloading...\")\n",
    "    urllib.request.urlretrieve(file_3_url, file_3_path)\n",
    "else:\n",
    "    print(f\"{file_3_path} already exists, skipping download.\")\n",
    "\n",
    "# Check if file 4 exists, if not, download it\n",
    "if not os.path.exists(file_4_path):\n",
    "    print(f\"{file_4_path} not found, downloading...\")\n",
    "    urllib.request.urlretrieve(file_4_url, file_4_path)\n",
    "else:\n",
    "    print(f\"{file_4_path} already exists, skipping download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9e7afa-83c3-4ff9-b0d9-d301a19153c8",
   "metadata": {},
   "source": [
    "## Generating dixit dataset for biolord"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa163030-0df2-4bfd-8abe-a2e0b266676a",
   "metadata": {},
   "source": [
    "In their work of `biolord`, the datasets `dixit` is not considered. Hence, we follow the similar preprocessing workflows and create the `dixit` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "747b1d15-7764-41ca-ad36-beb5217c9d6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/1751625666.py:16: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[f\"split{seed}\"] = [pert2set[i] for i in adata.obs[\"condition\"].values]\n",
      "/tmp/ipykernel_82/1751625666.py:36: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('target').apply(lambda x: x.nlargest(20 + 1, ['importance'])).reset_index(drop = True)\n",
      "/tmp/ipykernel_82/1751625666.py:57: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df_perts_expression = df_perts_expression.groupby([\"condition\"]).mean()\n",
      "/opt/conda/lib/python3.10/site-packages/anndata/_core/anndata.py:430: FutureWarning: The dtype argument is deprecated and will be removed in late 2024.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "adata = sc.read('../Data_GEARS/dixit/perturb_processed.h5ad')\n",
    "for seed in range(1,11):\n",
    "    with open(f\"../Data_GEARS/dixit/splits/dixit_simulation_{seed}_0.9.pkl\", \"rb\") as f:\n",
    "        split_data = pickle.load(f)\n",
    "        pert2set = {}\n",
    "        for i,j in split_data.items():\n",
    "            for x in j:\n",
    "                pert2set[x] = i\n",
    "        \n",
    "        not_in = np.setxor1d(list(adata.obs.condition.unique()), list(pert2set.keys()))\n",
    "        if len(not_in) > 0:\n",
    "            for i in not_in:\n",
    "                adata = adata[adata.obs.condition != i]\n",
    "                \n",
    "        subgroup = pickle.load(open(f\"../Data_GEARS/dixit/splits/dixit_simulation_{seed}_0.9_subgroup.pkl\", \"rb\"))\n",
    "        adata.obs[f\"split{seed}\"] = [pert2set[i] for i in adata.obs[\"condition\"].values]\n",
    "        pert2subgroup = {}\n",
    "        for i,j in subgroup[\"test_subgroup\"].items():\n",
    "            for x in j:\n",
    "                pert2subgroup[x] = i\n",
    "                \n",
    "        adata.obs[f\"subgroup{seed}\"] = adata.obs[\"condition\"].apply(lambda x: pert2subgroup[x] if x in pert2subgroup else 'Train/Val')\n",
    "        rename = {\n",
    "            'train': 'train',\n",
    "             'test': 'ood',\n",
    "             'val': 'test'\n",
    "        }\n",
    "        adata.obs[f'split{seed}'] = adata.obs[f'split{seed}'].apply(lambda x: rename[x])\n",
    "        \n",
    "adata.obs[\"perturbation\"] = [cond.split(\"+\")[0] for cond in adata.obs[\"condition\"]]\n",
    "adata.obs[\"perturbation\"] = adata.obs[\"perturbation\"].astype(\"category\")\n",
    "\n",
    "go_path = '../Data_GEARS/dixit/go.csv'\n",
    "gene_path = '../Data_GEARS/essential_all_data_pert_genes.pkl'\n",
    "df = pd.read_csv(go_path)\n",
    "df = df.groupby('target').apply(lambda x: x.nlargest(20 + 1, ['importance'])).reset_index(drop = True)\n",
    "with open(gene_path, 'rb') as f:\n",
    "    gene_list = pickle.load(f)\n",
    "df = df[df[\"source\"].isin(gene_list)]\n",
    "\n",
    "def get_map(pert):\n",
    "    tmp = pd.DataFrame(np.zeros(len(gene_list)), index=gene_list)\n",
    "    tmp.loc[df[df.target == pert].source.values, :] = df[df.target == pert].importance.values[:, np.newaxis]\n",
    "    return tmp.values.flatten()\n",
    "\n",
    "pert2neighbor =  {i: get_map(i) for i in list(adata.obs[\"perturbation\"].cat.categories)}\n",
    "adata.uns[\"pert2neighbor\"] = pert2neighbor\n",
    "\n",
    "pert2neighbor = np.asarray([val for val in adata.uns[\"pert2neighbor\"].values()])\n",
    "keep_idx = pert2neighbor.sum(0) > 0\n",
    "\n",
    "name_map = dict(adata.obs[[\"condition\", \"condition_name\"]].drop_duplicates().values)\n",
    "ctrl = np.asarray(adata[adata.obs[\"condition\"].isin([\"ctrl\"])].X.mean(0)).flatten() \n",
    "\n",
    "df_perts_expression = pd.DataFrame(adata.X.toarray(), index=adata.obs_names, columns=adata.var_names)\n",
    "df_perts_expression[\"condition\"] = adata.obs[\"condition\"]\n",
    "df_perts_expression = df_perts_expression.groupby([\"condition\"]).mean()\n",
    "df_perts_expression = df_perts_expression.reset_index()\n",
    "\n",
    "single_perts_condition = []\n",
    "single_pert_val = []\n",
    "double_perts = []\n",
    "for pert in adata.obs[\"condition\"].cat.categories:\n",
    "    if len(pert.split(\"+\")) == 1:\n",
    "        continue\n",
    "    elif \"ctrl\" in pert:\n",
    "        single_perts_condition.append(pert)\n",
    "        p1, p2 = pert.split(\"+\")\n",
    "        if p2 == \"ctrl\":\n",
    "            single_pert_val.append(p1)\n",
    "        else:\n",
    "            single_pert_val.append(p2)\n",
    "single_perts_condition.append(\"ctrl\")\n",
    "single_pert_val.append(\"ctrl\")\n",
    "\n",
    "df_singleperts_expression = pd.DataFrame(df_perts_expression.set_index(\"condition\").loc[single_perts_condition].values, index=single_pert_val)\n",
    "df_singleperts_emb = np.asarray([adata.uns[\"pert2neighbor\"][p1][keep_idx] for p1 in df_singleperts_expression.index])\n",
    "\n",
    "df_singleperts_condition = pd.Index(single_perts_condition)\n",
    "df_single_pert_val = pd.Index(single_pert_val)\n",
    "\n",
    "adata_single = ad.AnnData(X=df_singleperts_expression.values, var=adata.var.copy(), dtype=df_singleperts_expression.values.dtype)\n",
    "adata_single.obs_names = df_singleperts_condition\n",
    "adata_single.obs[\"condition\"] = df_singleperts_condition\n",
    "adata_single.obs[\"perts_name\"] = df_single_pert_val\n",
    "adata_single.obsm[\"perturbation_neighbors\"] = df_singleperts_emb\n",
    "\n",
    "for split_seed in range(1,11):\n",
    "    adata_single.obs[f\"split{split_seed}\"] = None\n",
    "    adata_single.obs[f\"subgroup{split_seed}\"] = \"Train/Val\"\n",
    "    for cat in [\"train\",\"test\",\"ood\"]:\n",
    "        cat_idx = adata_single.obs[\"condition\"].isin(adata[adata.obs[f\"split{split_seed}\"] == cat].obs[\"condition\"].cat.categories)\n",
    "        adata_single.obs.loc[cat_idx ,f\"split{split_seed}\"] = cat\n",
    "        if cat == \"ood\":\n",
    "            adata_single.obs.loc[cat_idx ,f\"subgroup{split_seed}\"] = \"unseen_single\"\n",
    "            \n",
    "adata_single.write(\"dixit/dixit_single_biolord.h5ad\")\n",
    "adata.write(\"dixit/dixit_biolord.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523198f6-eaf8-4019-8e54-92442ec4569e",
   "metadata": {},
   "source": [
    "## Generating dixit dataset for Replogle K562"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acfa8d5-406c-4737-b9b2-8d4af74a366a",
   "metadata": {},
   "source": [
    "Similarly, we follow the the preprocessing workflows and create the `Replogle K562` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ca64032-4295-4801-bc25-fc73d0bb6323",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/153567061.py:16: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[f\"split{seed}\"] = [pert2set[i] for i in adata.obs[\"condition\"].values]\n",
      "/tmp/ipykernel_82/153567061.py:36: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('target').apply(lambda x: x.nlargest(20 + 1, ['importance'])).reset_index(drop = True)\n",
      "/tmp/ipykernel_82/153567061.py:57: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df_perts_expression = df_perts_expression.groupby([\"condition\"]).mean()\n",
      "/opt/conda/lib/python3.10/site-packages/anndata/_core/anndata.py:430: FutureWarning: The dtype argument is deprecated and will be removed in late 2024.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "adata = sc.read('../Data_GEARS/replogle_k562_essential/perturb_processed.h5ad')\n",
    "for seed in range(1,6):\n",
    "    with open(f\"../Data_GEARS/replogle_k562_essential/splits/replogle_k562_essential_simulation_{seed}_0.75.pkl\", \"rb\") as f:\n",
    "        split_data = pickle.load(f)\n",
    "        pert2set = {}\n",
    "        for i,j in split_data.items():\n",
    "            for x in j:\n",
    "                pert2set[x] = i\n",
    "        \n",
    "        not_in = np.setxor1d(list(adata.obs.condition.unique()), list(pert2set.keys()))\n",
    "        if len(not_in) > 0:\n",
    "            for i in not_in:\n",
    "                adata = adata[adata.obs.condition != i]\n",
    "                \n",
    "        subgroup = pickle.load(open(f\"../Data_GEARS/replogle_k562_essential/splits/replogle_k562_essential_simulation_{seed}_0.75_subgroup.pkl\", \"rb\"))\n",
    "        adata.obs[f\"split{seed}\"] = [pert2set[i] for i in adata.obs[\"condition\"].values]\n",
    "        pert2subgroup = {}\n",
    "        for i,j in subgroup[\"test_subgroup\"].items():\n",
    "            for x in j:\n",
    "                pert2subgroup[x] = i\n",
    "                \n",
    "        adata.obs[f\"subgroup{seed}\"] = adata.obs[\"condition\"].apply(lambda x: pert2subgroup[x] if x in pert2subgroup else 'Train/Val')\n",
    "        rename = {\n",
    "            'train': 'train',\n",
    "             'test': 'ood',\n",
    "             'val': 'test'\n",
    "        }\n",
    "        adata.obs[f'split{seed}'] = adata.obs[f'split{seed}'].apply(lambda x: rename[x])\n",
    "        \n",
    "adata.obs[\"perturbation\"] = [cond.split(\"+\")[0] for cond in adata.obs[\"condition\"]]\n",
    "adata.obs[\"perturbation\"] = adata.obs[\"perturbation\"].astype(\"category\")\n",
    "\n",
    "go_path = '../Data_GEARS/go_essential_all/go_essential_all.csv'\n",
    "gene_path = '../Data_GEARS/essential_all_data_pert_genes.pkl'\n",
    "df = pd.read_csv(go_path)\n",
    "df = df.groupby('target').apply(lambda x: x.nlargest(20 + 1, ['importance'])).reset_index(drop = True)\n",
    "with open(gene_path, 'rb') as f:\n",
    "    gene_list = pickle.load(f)\n",
    "df = df[df[\"source\"].isin(gene_list)]\n",
    "\n",
    "def get_map(pert):\n",
    "    tmp = pd.DataFrame(np.zeros(len(gene_list)), index=gene_list)\n",
    "    tmp.loc[df[df.target == pert].source.values, :] = df[df.target == pert].importance.values[:, np.newaxis]\n",
    "    return tmp.values.flatten()\n",
    "\n",
    "pert2neighbor =  {i: get_map(i) for i in list(adata.obs[\"perturbation\"].cat.categories)}\n",
    "adata.uns[\"pert2neighbor\"] = pert2neighbor\n",
    "\n",
    "pert2neighbor = np.asarray([val for val in adata.uns[\"pert2neighbor\"].values()])\n",
    "keep_idx = pert2neighbor.sum(0) > 0\n",
    "\n",
    "name_map = dict(adata.obs[[\"condition\", \"condition_name\"]].drop_duplicates().values)\n",
    "ctrl = np.asarray(adata[adata.obs[\"condition\"].isin([\"ctrl\"])].X.mean(0)).flatten() \n",
    "\n",
    "df_perts_expression = pd.DataFrame(adata.X.toarray(), index=adata.obs_names, columns=adata.var_names)\n",
    "df_perts_expression[\"condition\"] = adata.obs[\"condition\"]\n",
    "df_perts_expression = df_perts_expression.groupby([\"condition\"]).mean()\n",
    "df_perts_expression = df_perts_expression.reset_index()\n",
    "\n",
    "single_perts_condition = []\n",
    "single_pert_val = []\n",
    "double_perts = []\n",
    "for pert in adata.obs[\"condition\"].cat.categories:\n",
    "    if len(pert.split(\"+\")) == 1:\n",
    "        continue\n",
    "    elif \"ctrl\" in pert:\n",
    "        single_perts_condition.append(pert)\n",
    "        p1, p2 = pert.split(\"+\")\n",
    "        if p2 == \"ctrl\":\n",
    "            single_pert_val.append(p1)\n",
    "        else:\n",
    "            single_pert_val.append(p2)\n",
    "single_perts_condition.append(\"ctrl\")\n",
    "single_pert_val.append(\"ctrl\")\n",
    "\n",
    "df_singleperts_expression = pd.DataFrame(df_perts_expression.set_index(\"condition\").loc[single_perts_condition].values, index=single_pert_val)\n",
    "df_singleperts_emb = np.asarray([adata.uns[\"pert2neighbor\"][p1][keep_idx] for p1 in df_singleperts_expression.index])\n",
    "\n",
    "df_singleperts_condition = pd.Index(single_perts_condition)\n",
    "df_single_pert_val = pd.Index(single_pert_val)\n",
    "\n",
    "adata_single = ad.AnnData(X=df_singleperts_expression.values, var=adata.var.copy(), dtype=df_singleperts_expression.values.dtype)\n",
    "adata_single.obs_names = df_singleperts_condition\n",
    "adata_single.obs[\"condition\"] = df_singleperts_condition\n",
    "adata_single.obs[\"perts_name\"] = df_single_pert_val\n",
    "adata_single.obsm[\"perturbation_neighbors\"] = df_singleperts_emb\n",
    "\n",
    "for split_seed in range(1,6):\n",
    "    adata_single.obs[f\"split{split_seed}\"] = None\n",
    "    adata_single.obs[f\"subgroup{split_seed}\"] = \"Train/Val\"\n",
    "    for cat in [\"train\",\"test\",\"ood\"]:\n",
    "        cat_idx = adata_single.obs[\"condition\"].isin(adata[adata.obs[f\"split{split_seed}\"] == cat].obs[\"condition\"].cat.categories)\n",
    "        adata_single.obs.loc[cat_idx ,f\"split{split_seed}\"] = cat\n",
    "        if cat == \"ood\":\n",
    "            adata_single.obs.loc[cat_idx ,f\"subgroup{split_seed}\"] = \"unseen_single\"\n",
    "            \n",
    "adata_single.write(\"replogle_k562_essential/k562_single_biolord.h5ad\")\n",
    "adata.write(\"replogle_k562_essential/k562_biolord.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc26c5e5-b1e0-42b1-942d-c1ea0a0f1146",
   "metadata": {},
   "source": [
    "## Generating dixit dataset for Replogle K562"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a338049-8af3-4d91-95af-61340ef3cb30",
   "metadata": {},
   "source": [
    "Similarly, we follow the the preprocessing workflows and create the `Replogle RPE1` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d32df1e-3251-48e6-8f90-c65e47627621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82/1690946857.py:16: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata.obs[f\"split{seed}\"] = [pert2set[i] for i in adata.obs[\"condition\"].values]\n",
      "/tmp/ipykernel_82/1690946857.py:36: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('target').apply(lambda x: x.nlargest(20 + 1, ['importance'])).reset_index(drop = True)\n",
      "/tmp/ipykernel_82/1690946857.py:57: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  df_perts_expression = df_perts_expression.groupby([\"condition\"]).mean()\n",
      "/opt/conda/lib/python3.10/site-packages/anndata/_core/anndata.py:430: FutureWarning: The dtype argument is deprecated and will be removed in late 2024.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "adata = sc.read('../Data_GEARS/replogle_rpe1_essential/perturb_processed.h5ad')\n",
    "for seed in range(1,6):\n",
    "    with open(f\"../Data_GEARS/replogle_rpe1_essential/splits/replogle_rpe1_essential_simulation_{seed}_0.75.pkl\", \"rb\") as f:\n",
    "        split_data = pickle.load(f)\n",
    "        pert2set = {}\n",
    "        for i,j in split_data.items():\n",
    "            for x in j:\n",
    "                pert2set[x] = i\n",
    "        \n",
    "        not_in = np.setxor1d(list(adata.obs.condition.unique()), list(pert2set.keys()))\n",
    "        if len(not_in) > 0:\n",
    "            for i in not_in:\n",
    "                adata = adata[adata.obs.condition != i]\n",
    "                \n",
    "        subgroup = pickle.load(open(f\"../Data_GEARS/replogle_rpe1_essential/splits/replogle_rpe1_essential_simulation_{seed}_0.75_subgroup.pkl\", \"rb\"))\n",
    "        adata.obs[f\"split{seed}\"] = [pert2set[i] for i in adata.obs[\"condition\"].values]\n",
    "        pert2subgroup = {}\n",
    "        for i,j in subgroup[\"test_subgroup\"].items():\n",
    "            for x in j:\n",
    "                pert2subgroup[x] = i\n",
    "                \n",
    "        adata.obs[f\"subgroup{seed}\"] = adata.obs[\"condition\"].apply(lambda x: pert2subgroup[x] if x in pert2subgroup else 'Train/Val')\n",
    "        rename = {\n",
    "            'train': 'train',\n",
    "             'test': 'ood',\n",
    "             'val': 'test'\n",
    "        }\n",
    "        adata.obs[f'split{seed}'] = adata.obs[f'split{seed}'].apply(lambda x: rename[x])\n",
    "        \n",
    "adata.obs[\"perturbation\"] = [cond.split(\"+\")[0] for cond in adata.obs[\"condition\"]]\n",
    "adata.obs[\"perturbation\"] = adata.obs[\"perturbation\"].astype(\"category\")\n",
    "\n",
    "go_path = '../Data_GEARS/go_essential_all/go_essential_all.csv'\n",
    "gene_path = '../Data_GEARS/essential_all_data_pert_genes.pkl'\n",
    "df = pd.read_csv(go_path)\n",
    "df = df.groupby('target').apply(lambda x: x.nlargest(20 + 1, ['importance'])).reset_index(drop = True)\n",
    "with open(gene_path, 'rb') as f:\n",
    "    gene_list = pickle.load(f)\n",
    "df = df[df[\"source\"].isin(gene_list)]\n",
    "\n",
    "def get_map(pert):\n",
    "    tmp = pd.DataFrame(np.zeros(len(gene_list)), index=gene_list)\n",
    "    tmp.loc[df[df.target == pert].source.values, :] = df[df.target == pert].importance.values[:, np.newaxis]\n",
    "    return tmp.values.flatten()\n",
    "\n",
    "pert2neighbor =  {i: get_map(i) for i in list(adata.obs[\"perturbation\"].cat.categories)}\n",
    "adata.uns[\"pert2neighbor\"] = pert2neighbor\n",
    "\n",
    "pert2neighbor = np.asarray([val for val in adata.uns[\"pert2neighbor\"].values()])\n",
    "keep_idx = pert2neighbor.sum(0) > 0\n",
    "\n",
    "name_map = dict(adata.obs[[\"condition\", \"condition_name\"]].drop_duplicates().values)\n",
    "ctrl = np.asarray(adata[adata.obs[\"condition\"].isin([\"ctrl\"])].X.mean(0)).flatten() \n",
    "\n",
    "df_perts_expression = pd.DataFrame(adata.X.toarray(), index=adata.obs_names, columns=adata.var_names)\n",
    "df_perts_expression[\"condition\"] = adata.obs[\"condition\"]\n",
    "df_perts_expression = df_perts_expression.groupby([\"condition\"]).mean()\n",
    "df_perts_expression = df_perts_expression.reset_index()\n",
    "\n",
    "single_perts_condition = []\n",
    "single_pert_val = []\n",
    "double_perts = []\n",
    "for pert in adata.obs[\"condition\"].cat.categories:\n",
    "    if len(pert.split(\"+\")) == 1:\n",
    "        continue\n",
    "    elif \"ctrl\" in pert:\n",
    "        single_perts_condition.append(pert)\n",
    "        p1, p2 = pert.split(\"+\")\n",
    "        if p2 == \"ctrl\":\n",
    "            single_pert_val.append(p1)\n",
    "        else:\n",
    "            single_pert_val.append(p2)\n",
    "single_perts_condition.append(\"ctrl\")\n",
    "single_pert_val.append(\"ctrl\")\n",
    "\n",
    "df_singleperts_expression = pd.DataFrame(df_perts_expression.set_index(\"condition\").loc[single_perts_condition].values, index=single_pert_val)\n",
    "df_singleperts_emb = np.asarray([adata.uns[\"pert2neighbor\"][p1][keep_idx] for p1 in df_singleperts_expression.index])\n",
    "\n",
    "df_singleperts_condition = pd.Index(single_perts_condition)\n",
    "df_single_pert_val = pd.Index(single_pert_val)\n",
    "\n",
    "adata_single = ad.AnnData(X=df_singleperts_expression.values, var=adata.var.copy(), dtype=df_singleperts_expression.values.dtype)\n",
    "adata_single.obs_names = df_singleperts_condition\n",
    "adata_single.obs[\"condition\"] = df_singleperts_condition\n",
    "adata_single.obs[\"perts_name\"] = df_single_pert_val\n",
    "adata_single.obsm[\"perturbation_neighbors\"] = df_singleperts_emb\n",
    "\n",
    "for split_seed in range(1,6):\n",
    "    adata_single.obs[f\"split{split_seed}\"] = None\n",
    "    adata_single.obs[f\"subgroup{split_seed}\"] = \"Train/Val\"\n",
    "    for cat in [\"train\",\"test\",\"ood\"]:\n",
    "        cat_idx = adata_single.obs[\"condition\"].isin(adata[adata.obs[f\"split{split_seed}\"] == cat].obs[\"condition\"].cat.categories)\n",
    "        adata_single.obs.loc[cat_idx ,f\"split{split_seed}\"] = cat\n",
    "        if cat == \"ood\":\n",
    "            adata_single.obs.loc[cat_idx ,f\"subgroup{split_seed}\"] = \"unseen_single\"\n",
    "            \n",
    "adata_single.write(\"replogle_rpe1_essential/rpe1_single_biolord.h5ad\")\n",
    "adata.write(\"replogle_rpe1_essential/rpe1_biolord.h5ad\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
